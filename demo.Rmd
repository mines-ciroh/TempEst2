---
title: "TempEst 2 Demo"
author: "Daniel Philippus"
date: "`r format(Sys.Date(), format='%b %d, %Y)')`"
output: pdf_document
---

This is a quick demo of how to use the TempEst 2 model code.  Dependencies are
`tidyverse` and `fields`, both imported by `model.R`.

```{r setup, warning=FALSE, message=FALSE}
source("model.R")
```


# Earth Engine Data Retrieval

TempEst 2 comes with a Python script for automatically retrieving predictor
data from Google Earth Engine.  This requires that you have a Google Earth Engine
account and have local Earth Engine access set up for Python on your computer,
though it should be straightforward to modify for use in Google Collaboratory.

For that script, it is simply necessary to specify a date range of interest and
points of interest.  Examples are included for the format of the points of interest.
Then, the function `getAllTimeseries()` will retrieve a series of CSVs (one
for each timestep) in Google Earth Engine.  These CSVs should be joined together
into one data frame **before** making predictions, as TempEst 2 needs to analyze
long-term data to generate seasonality estimates.  Otherwise, the only modification
is to add a `day` column (e.g., `as.integer(format(date, "%j")))`).

```{r}
data <- map_dfr(
  list.files("GEEData/", full.names = T),
  \(fn) read_csv(fn, col_types="cDddddddddd") %>%
    mutate(day = as.integer(format(date, "%j"))) %>%
    filter(lst > -273)  # filters errors
)
```


# Prediction with a Trained Model

TempEst 2 comes with a pretrained model, `model.rda`.  Loading this RData file
will create a `model` function.  This function takes in the predictor data frame
and adds the columns `temp.doy` (day-of-year mean temperature), `temp.anom`
(daily anomaly relative to day-of-year mean, i.e., de-seasoned), and `temp.mod`
(overall modeled temperature for each day, the sum of the previous two).

```{r message=FALSE}
load("model.rda")
pred <- model(data) %>% drop_na
```

```{r message=FALSE}
pred %>%
  group_by(id, lon, lat) %>%
  summarize(
    meanT = mean(temp.mod),
    range = max(temp.mod) - min(temp.mod)
  ) %>%
  ggplot(aes(lon, lat, size=range, color=meanT)) +
  geom_point() +
  scale_size_continuous(range=c(2, 10)) +
  scale_color_viridis_c(option = "magma") +
  theme_bw() +
  labs(
    x = "Longitude",
    y = "Latitude",
    color = "Mean Temperature (C)",
    size = "Temperature Range (C)",
    title = "Colorado Prediction Points"
  )
```

# Training a Model

The data requirements are the same, except the data must also have a `temperature`
column for daily mean temperature.  For TempEst 2 development, this was retrieved
from USGS gages.

Using a hypothetical `indat`, then, just run:

```model <- full.schema()(indat)```

Note the empty parentheses; the arguments to `full.schema` are the seasonality
and anomaly model functions to use (defaults provided), and then it returns
another function, which takes training data as an argument, which returns a
model function as above.  This means it is possible to use your own functions
for either of those components.

## Full Demo

For building the PDF, this demo used pre-downloaded data which is too big for
GitHub.  However, the process is shown below, assuming GEE data have been downloaded
to a `GEEData` folder and have IDs matched to USGS gage IDs.

For testing, these data had already been downloaded to `GageTemps.csv`, so that
portion is not run.

```{r}
library(tidyverse)
```


### USGS Data Downloading

Gage data retrieval can fail due to network issues, so it's advisable
to run this in a way that makes it easy to go back and try again.  That is ignored
here.  Running with a timeout is also helpful (`withTimeout` from `R.utils`).

Writing incrementally, instead of collecting a single data frame, helps with
keeping memory use down over many gages and allows the process to be resumed
if it crashes.

```{r}
library(dataRetrieval)
gage.ids <- c("10343500")  # provide a list of IDs here... this one is Sagehen Creek, CA.
gage.file <- "GageDataDemo.csv"
start <- "2001-01-01"
end <- "2023-12-31"
var.temp <- "00010"

walk(gage.ids,
     \(gid) {
       readNWISdv(gid, var.temp, start, end) %>%
         select(id = site_no,
                date = Date,
                temperature = X_00010_00003) %>%
         mutate(year = as.integer(format(date, "%Y")),
                day = as.integer(format(date, "%j"))) %>%
         select(id, date, year, day, temperature) %>%
         write_csv(gage.file, append = file.exists(gage.file))
     })
```

```{r}
gage.data <- read_csv(gage.file)
```

```{r}
ggplot(gage.data, aes(date, temperature)) +
  geom_line() +
  theme_bw() +
  labs(
    x="Date",
    y="Daily Mean Temperature (C)"
  )
```


### Load Gage Data

This loads the actual gage data used for analysis, not the demo above.  This has
some technical debt, so it's handled a bit differently to convert it into the
equivalent format above.  There are 6 million observations over 22 years at 1,442
gages, after removing obviously erroneous observations (temperature > 100 or < 0).

```{r}
gage.data <- read_csv("GageTemp.csv", col_types = "cicd") %>%
  extract(time, "day", "([0-9]+)", convert=TRUE) %>%
  mutate(
    date = as.Date(paste0(year, day), format="%Y %j")
  ) %>%
  select(id, date, temperature) %>%
  filter(temperature >= 0, temperature <= 100)
summary(gage.data)
length(unique(gage.data$id))
```

### Load Spatial Data

This has different column structure than the example above, since it was the intial
analysis dataset and variables were retrieved which later turned out to be irrelevant.

The spatial data has 4.6M observations for 1506 points and 22 years.

```{r eval=FALSE}
spat.data <-
  map_dfr(
    list.files("FullGEE/", pattern = "AllPoints*", full.names = T),
    ~ read_csv(., col_types = "cDddddddddddddddddddd")
  ) %>%
  select(id:elevation,
         lst = lst_day, humidity,
         water, shrubland, grassland, barren) %>%
  mutate(year = as.integer(format(date, "%Y")),
         day = as.integer(format(date, "%j"))) %>%
  filter(lst > -273, humidity >= 0) %>%  # remove missing data
  drop_na()
write_csv(spat.data, "SpatialData.csv")
```
```{r}
spat.data <- read_csv("SpatialData.csv",
                      col_types = "cDddddddddddii") %>%
  # There are two redundant gages where an old gage was replaced.
  # Kriging does not work with non-unique points.  Easy solution: remove the
  # older gage.
  filter(
    !(id %in% c("420853121505500", "421209121463000"))
  )
```
```{r}
summary(spat.data)
length(unique(spat.data$id))
```


### Combine Data

For training the model, do *not* remove NAs from the combined dataset initially.
The model uses a variety of monthly and long-term means in the spatial data, so
spatial coverage that is not matched to gage coverage remains valuable, and gage
coverage not matched to spatial coverage is useful for fitting seasonal conditions.

The full dataset contains ~8.4 million observations, but only ~2.2 million of those
have fully overlapping coverage.  Many gages do not have continuous coverage for
the full period of interest, and it is nearly guaranteed that some days and points
will not have satellite coverage (cloudy).

```{r eval=FALSE}
data <- full_join(spat.data, gage.data,
                  by=c("id", "date"))
write_csv(data, "AllData.csv")
```
```{r}
data <- read_csv("AllData.csv", col_types = "cDdddddddddiid")
```

### Building a Model

Usually, the objective would be to build a predictive model, in which case we
can leave the arguments to `full.schema` empty (defaults). In this case, we
add the `rtn.model=TRUE` argument, which returns a list of the model components
instead so those can be inspected.  This version does not support prediction.

The default model components are several kriging models, so we can inspect
meaningful coefficients directly.  For example, this could be used to retrieve
the trend in mean annual temperature with respect to river width (approximated by
water abundance), all else being equal.

```{r warning=FALSE, message=FALSE}
model <- full.schema(rtn.model=TRUE, use.max=TRUE)(data)
save(model, file="krigs.rda")
```

To build one that supports prediction:

```{r eval=FALSE}
model <- full.schema()(data)
save(model, file="model.rda")
```

### Inspecting Model Components

Using `rtn.model=TRUE`, each entry in the list is a kriging model.  Some items
of interest are shown below, followed by code to summarize model parameters.

```{r}
load("krigs.rda")
```


```{r}
model$Intercept$summary
summary(model$Intercept)$fixedEffectsTable  # intercept, lon, lat, then fixed terms
colnames(model$Intercept$Z)
```


```{r}
modstats <- map_dfr(model,
                    ~as_tibble_row(.x$summary),
                    .id = "Component")
write_csv(modstats, "SpatialModel.csv")
select(modstats, Component, aRange, sigma2, lambda, tau)
summary(select(modstats, aRange, sigma2, lambda, tau))
```

```{r}
modcoefs <- map_dfr(model,
                    \(mod) {
                      as_tibble(summary(mod)$fixedEffectsTable) %>%
                        mutate(Variable = c("Intercept", "lon", "lat",
                                            colnames(mod$Z))) %>%
                        relocate(Variable)
                    },
                    .id = "Component")
write_csv(modcoefs, "FixedModel.csv")
modcoefs
```

#### Plot Spatial Signal

`fields` includes functions to directly plot a grid of the spatial signal in
a spatial-statistical model.  In effect, this displays spatial anomalies, where
there is a spatial autocorrelation in the component of the model that is not
explained by the fixed regression - in other words, where the sites in a region
tend to behave weirdly (distinct from random noise at an individual site).
This suggests where there may be non-modeled phenomena impacting stream temperature.

The plots are a bit difficult to see embedded in the notebook, so to summarize:

- Intercept (mean temperature) basically has a north-south gradient with scattered
short-distance anomalies.
- Amplitude (annual variation) increases generally to the northeast, but has
several identifiable anomaly regions: northern Arkansas, northern Florida,
northern Arizona, eastern California, and northwestern Oregon.  Several of these
correspond to cross-validation error hot spots.
- The autumn/winter coefficient weakly increases to the west, but the range is
dominated by spatial signal.  There are several high points in the west and low
points in the east.
- The spring/summer coefficient generally increases to the northwest.  Anomalies
are mostly short-ranged, but there is a bit of a general high point in the northern
Rockies/Cascades region.
- The peak winter date has very little discernible gradient; it is dominated by
longer-ranged spatial anomalies.  These are notably present along the West Coast
mountains (early) and near the Great Lakes area (late), along with many weaker
anomalies.
- LST sensitivity increases to the north and a little to the west.  There are
no visible anomalies.
- Humidity sensitivity increases to the west and a little to the north.  There
are no visible anomalies.

```{r}
usa <- map_data("state")

exmod <- model$Humidity

# 25 to 50 N, 125 to 65 W
gList <- list(
  lon = (-125):(-65),
  lat = 25:55
)

# drop.Z: ignore fixed-part covariates
surf <- predictSurface(exmod, gridList = gList, drop.Z = TRUE, extrap = TRUE)
sg <- as_tibble(make.surface.grid(gList)) %>%
  mutate(z = as.vector(surf$z))


ggplot(sg, aes(x=lon, y=lat, fill=z)) +
  geom_tile() +
  geom_polygon(aes(long, lat, group=group, fill=NA), data=usa, color="black",
               fill=NA) +
  theme_bw()
```

Now map it over all of them.

```{r}
spatsig <- map_dfr(model,
                   ~{
                     as_tibble(make.surface.grid(gList)) %>%
                       mutate(z = as.vector(predictSurface(.x,
                                                           gridList = gList,
                                                           drop.Z=TRUE
                                                           )$z))
                   }, .id = "Component")
```

```{r}
namer <- c(
  "Amplitude" = "(B) Annual Amplitude",
  "AutumnWinter" = "(C) Autumn/Winter Coefficient",
  "Humidity" = "Daily Mean Humidity Coefficient",
  "HumidityMax" = "Daily Max. Humidity Coefficient",
  "Intercept" = "(A) Mean Temperature",
  "InterceptMax" = "Daily Max. Intercept",
  "LST" = "Daily Mean LST Coefficient",
  "LSTMax" = "Daily Max. LST Coefficient",
  "SpringSummer" = "Spring/Summer Coefficient",
  "WinterDay" = "(D) Winter Anomaly Peak Date"
)
spatsig %>%
  drop_na() %>%
  filter(Component %in% c(
    "WinterDay", "Amplitude", "Intercept", "AutumnWinter"
  )) %>%
  mutate(Component = namer[Component]) %>%
  group_by(Component) %>%
  mutate(z = (z - min(z, na.rm=T)) / (max(z, na.rm=T) - min(z, na.rm=T))) %>%
  ggplot(aes(lon, lat, fill=z)) +
  geom_tile() +
  scale_fill_viridis_c() +
  geom_polygon(aes(long, lat, group=group, fill=NA), data=usa, fill=NA, color="black") +
  facet_wrap(~Component, ncol=2) +
  theme_bw() +
  theme(legend.position = "bottom") +
  labs(
    x="Longitude",
    y="Latitude",
    fill="Normalized Spatial Component"
  )

ggsave("Figures/SpatialSignal.png", width=6.5, height=5, bg="white")
```



#### Export Spatial Signal

We can export the spatial signals as a NetCDF for use in TempEst 2-FAST.  We
extend it a few degrees up into Canada so it can be used for the Columbia and
Great Lakes watersheds.

The exporting is broken into chunks because full-CONUS, high-resolution runs use
enormous RAM.

```{r}
library(terra)
load("krigs.rda")
projstr <- "+proj=longlat +datum=WGS84"
```

```{r}
# 1 km resolution = approx. 1/100 degree
# 6001 x 3001
# For testing, we can use low resolution, because a higher-res run uses many
# GB of RAM.
# Additionally, 1-km resolution takes excessive space, in general. Try 5-km, which should still have
# little impact.
abs_min_lat <- 25
abs_max_lat <- 55
abs_min_lon <- -125
abs_max_lon <- -65
lon_size <- abs_max_lon - abs_min_lon
lat_size <- abs_max_lat - abs_min_lat
scale <- 20
chunks <- 100  # in each direction, so total chunks = N^2
spr <- NULL
tictoc::tic()
for (xc in 1:chunks - 1) {
  # Determine whether to exclude the rightmost column (avoid overlap).
  min_lon = abs_min_lon + xc * lon_size / chunks
  max_lon = abs_min_lon + (xc + 1) * lon_size / chunks - 1/scale  # exclude the right
  for (yc in 1:chunks - 1) {
    min_lat = abs_min_lat + yc * lat_size / chunks
    max_lat = abs_min_lat + (yc + 1) * lat_size / chunks - 1/scale  # exclude the right
    gList <- list(
      lon = (min_lon * scale):(max_lon * scale) / scale,
      lat = (min_lat * scale):(max_lat * scale) / scale
    )
    nr <- length(gList$lat)
    spatraster <- rast(ncol = length(gList$lon),
                      nrow = nr,
                      xmin = min_lon,
                      xmax = max_lon,
                      ymin = min_lat,
                      ymax = max_lat,
                      nlyr = 10,
                      names = names(model))

    # Fields counts from bottom to top, terra counts from top to bottom
    for (nm in names(model)) {
      values(spatraster[[nm]]) <- t(predictSurface(model[[nm]], gridList = gList,
                                            drop.Z = TRUE, extrap = TRUE)$z)[nr:1,]
    }
    if (is.null(spr)) {
      spr <- spatraster
    } else {
      # Build up incrementally
      spr <- mosaic(spr, spatraster)
    }
  }
  cat("|")
}
tictoc::toc()
writeCDF(spr, "SpatialModels.nc", overwrite = TRUE, split = TRUE)
plot(spr)
```



#### Plot Fixed Effects

Fixed effect coefficients from before:

```{r}
modcoefs
```

Now extract variable ranges:

```{r}
mc.range <- modcoefs %>%
  filter(Variable != "Intercept") %>%
  rowwise() %>%
  group_modify(~{
    cmp <- first(.x$Component)
    vr <- first(.x$Variable)
    vals <- cbind(model[[cmp]]$Z, model[[cmp]]$x)[,vr]
    y <- model[[cmp]]$y
    mutate(.x,
           YMin = min(y),
           YMax = max(y),
           Min = quantile(vals, 0.05),
           Max = quantile(vals, 0.95))
  }) %>%
  ungroup() %>%
  mutate(
    Effect = estimate * (Max - Min) / (YMax - YMin),
    LogEffect = sign(Effect) * log(abs(Effect) + 1)
  )
```

```{r}
ggplot(mc.range, aes(Variable, Component, fill=Effect)) +
  geom_tile() +
  scale_fill_viridis_c(limits=c(-5, 5), oob=scales::squish) +
  theme_bw() +
  theme(axis.text.x = element_text(angle=90, vjust=0.5)) +
  labs(
    x = "Feature",
    y = "Output",
    fill = "Normalized\nEffect"
  )
ggsave("Figures/LinearEffects.png", width=5, height=3, bg="white")
```

It is important to note that this is *not* showing standalone variable importance,
and key variables may act in combination.  Anywhere that effects are larger than 1,
meaning the effect of the range of the variable exceeds the actual range of the
prediction value, another variable must be counterbalancing it in practice.  For
example, large impacts across humidities suggest that what is happening is probably
a difference between humidities.

